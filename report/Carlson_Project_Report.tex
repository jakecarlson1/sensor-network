\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{csvsimple}
\usepackage{listings}
\usepackage{color}
\usepackage{geometry}
\geometry{
    letterpaper,
    left=1in,
    right=1in,
    top=1in,
    bottom=1in
}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{1,1,1}
\definecolor{gray}{rgb}{0.75, 0.75, 0.75}

\lstdefinestyle{codestyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=4
}

\begin{document}

\title{Backbone Determination in a Wireless Sensor Network}
\author{Jake Carlson}
\date{February 18, 2018}
\maketitle

\abstract
A report on implementing algorithms to partition a random geometric graph into bipartite subgraphs. Three different graph geometries are explored: unit square, unit disk, and unit sphere. Nodes are uniformly distributed in the geometry. Then the edges are determined and the verticies are colored using smallest-last vertex ordering.
\newpage

\tableofcontents
\lstlistoflistings
\newpage

\section{Executive Summary}

    \subsection{Introduction}
    Random geometric graphs (RGGs) are useful for simulating wireless sensor networks placed in different topologies. This project examines three different geometries: Square, Disk, and Sphere. The user supplies parameters for how many nodes they want in the network and how many connections they want for each node. Then, the simulation finds the average radius needed for that number of connections, finds multiple backbones for the RGG, and displays the results graphically.

    \subsection{Environment Description}
    The data structures and topologies for this simulation are implemented in Python2.7. The graphics are done using Processing.py. All development and benchmarking has been done on a 2014 MackBook Pro with a 3 GHz Intel Core i7 processor and 16 GB of DDR3 RAM running macOS High Sierra 10.13.3.
    \par
    I elected to use Processing because of the simple API used to draw and render shapes in two- and three-dimensional space. I choose to use Processing.py over Java Processing because of my familiarity and comfort with the Python programming language. I could have writen the graph generation, coloring, and backbone determination in another langague and saved the data to a file for loading in processing, but since I intend to implement all of the algorithms in linear time, I don't think there will be a performance issue with using an interpreted language instead of a compiled one.
    \par
    A separate data generation script was used to generate the summary table and graphs using matplotlib. This library, and a variety of others, could not be imported into Processing.py because the jython interpreter used by Processing only accepts libraries written in raw Python.
    \par
    The different geometries were implemented in a stand alone Python file and imported into the Processing.py script or the data generation script depending on what was being run. These classes can then be used directly by Processing or the data generation script. Because there is no intermediary file to hold the generated nodes and edges, there is no additional disk space needed to run the simulation. Everything can be done in system memory managed by Processing.

    \begin{center}
        \begin{table}
            \centering
            \csvautotabular{./data/benchmark-data-1.csv}
            \caption{Benchmarks for Generating RGGs}
            \label{tab1}
        \end{table}
    \end{center}

    \begin{center}
        \begin{table}
            \centering
            \csvautotabular{./data/benchmark-data-2.csv}
            \caption{Benchmarks for Coloring RGGs}
            \label{tab2}
        \end{table}
    \end{center}

\section{Reduction to Practice}

    \subsection{Data Structure Design}
    The primary data structure used for this project is an adjacency list. However, to allow for constant time lookup of edges of a node, I used a Python dictionary where the keys are nodes and the values are a list of indicies of adjacent nodes in the original list of nodes. The space needed by the adjacency list is $\Theta(|V| + 2|E|)$. Two entries are used for each edge because they are undirected. This is superior to the adjacency matrix data structure which would require $\Theta(|E|^2)$ space.
    \par
    In order to make this project maintainable as it is developed along the semester, I used the object-oriented capabilities Python has to offer to design the different geometries. I start with a base Topology class that creates the interface Processing uses to draw the graphs. This base class implements all of the methods needed for node placement and edge detection in 2D graphs. Then, I create three subclasses: Square, Disk, and Sphere.
    \par
    The Square and Disk topologies simply need to override the methods for generating nodes and calculating the node radius needed for the desired average degree. The Sphere subclass needs to override a few additional functions because it exists in a 3D space. Other than the methods for generating nodes and calculating the node radius, it also needs to override the function used to draw the graph so that Processing will render the graph properly in 3D.

    \subsection{Algorithm Descriptions}

        \subsubsection{Node Placement}
        A different node placement algorithm is required for each of the geometries. For the Square, the coordinates for each node are generated as two random numbers taken from a unifrom distribution on the range $[0,1]$. All of these points are guaranteed to be in the unit square.
        \par
        For the Disk, a similar method is used. The coordinates for nodes are randomly sampled from a uniform distribution; however, if a node has a distance from the center of the Disk greater than the radius of 1, the coordinates for that node are resampled.
        \par
        For the Sphere a different method must be used so that all of the nodes are placed on the surface of the Sphere and the volume is vacant. For this geometry, I used the following equations:
        \begin{align}
            x &= \sqrt{1-u^2}\cos\theta \\
            y &= \sqrt{1-u^2}\sin\theta \\
            z &= u
        \end{align}
        where $\theta \in [0,2\pi]$ and $u \in [-1,1]$. This is guaranteed to uniformly distribute nodes on the surface area of the sphere \cite{spherepoints}.
        \par
        All of these algorithms can be solved in $\Theta\left(|V|\right)$ where because each node only needs to be assigned a position once.

        \subsubsection{Edge Determination}
        There are several methods for finding the edges in the graph. The brute force method checks every node, and for each node checks all other nodes to see if they are close enough to form an edge. The brute force method is $\Theta\left(|V|^2\right)$.
        \par
        The second method to find the edges is the sweep method. This method first sorts the nodes along the x-axis. Then, for any node, we only need to search left and right until the distance along the x-axis is greater than the connection radius for the nodes. This dramatically reduces the search space. The sweep method is $O\left(n lg(n) + 2rn^2\right)$ where $n = |V|$ an $r$ is the connection radius. The $n lg(n)$ portion is for the sorting and the $2rn^2$ portion is for measuring the distance between nodes in a sweep step.
        \par
        The final method to find edges is the cell method. This method places the nodes into cells of area $r \times r$ based on their position in the topology. When the edge detection runs, each node needs to be visited once, but only the cell the node populates and the neighboring cells need to be searched for connections.
        \par
        The only method that needs to be adjusted for the Sphere is the cell method. Instead of using a two dimensional grid of cells, a three dimentional mesh is needed to divide the topology. The cells then have volume $r \times r \times r$. Only the current cell and the neighboring cells need to be searched.

        \subsubsection{Graph Coloring}
        Two algorithms are used for coloring the graphs. The first is smallest-last vertex ordering, which sorts the verticies based on the number of degrees they have. The second is the greedy graph coloring algorithm.
        \par
        Smallest-last vertex ordering is used to order the nodes for coloring. The steps to this algorithm are as follows \cite{slv}:
        \begin{enumerate}
            \item Initialize a representation of your target graph
            \item Find the vertex $v_j$ of minimum degree in your representation
            \item Update your representation to simulate deleting $v_j$
            \item If there are still verticies in the representation, return to step 1, otherwise terminate with the sequence of verticies removed
        \end{enumerate}
        This algorithm is linear if each of the above steps is linear. Step 1 is linear if we can build a represenation of the graph in linear time. For this, we can use an array of buckets, where each bucket holds the verticies that have the same number of edges as the position of the bucket in the array of buckets. To build this data structure, each node only needs to be visited once, making this linear in both space and time. Next, finding the vertex of minimum degree simply requires finding the lowest index bucket that has a node. This is bounded by the number of buckets, which is bounded by the number of nodes, making Step 2 linear. Next, we have to update the representation of the graph. To do this, we have to look at each node that shares an edge with $v_j$ and move it to the bucket for nodes with one fewer degree. This requires traversing the list of edges for $v_j$ which means Step 3 is linear. Since this is repeated for each node, the runtime of this program is $\Theta\left(|E| + |V|\right)$ and the space needed is $\Theta(|V|)$.
        \par
        After this, a single traversal of the smallest-last vertex ordering is needed to color the graph. As we traverse this list, we check to see if the nodes before it (that are already colored) share an edge with the current node. The node can then be colored with any color it does not share an edge with or, if it shares an edge with all currently used colors, it is assigned a new color. This algorithm is also linear. Each node needs to be visited once and when a node is visited, all previous nodes are checked to see if they are in the edge list of the current node. Because we used smallest last vertex ordering, as we have to check more and more nodes, we get to check fewer and fewer edges. This makes the greedy coloring algorithm $O(|V| + |E|)$.

    \subsection{Algorithm Engineering}

        \subsubsection{Node Placement}
        It is easy to implement the algorithms for placing nodes in the different geometries using Python's math library. This library offers functions for sampling points on a uniform distribution. For the Square, sampling on a range $[0,1]$ is sufficient for all of the nodes. Since each node only needs to be placed once, this runs at $\Theta(|V|)$ where.
        \par
        For the Disk, the node needs to be resampled if it is too far from the center. To do this, the distance function is used to find the distance between the node and the center. If the node is further than $1$ from the center, node generation falls into a while loop which iterates until the node is within the unit circle. Since nodes are taken from a uniform distribution, the number of nodes that will need to be resampled is approximately equal to the ratio of the area of the square that circumscribes the unit circle which falls outside of the unit circle to the total area of the square. This is given by:
        \begin{align}
            \frac{(2r)^2-\pi r^2}{(2r)^2} = \frac{4-\pi}{4} = 0.2146
        \end{align}
        Since the placement algorithm for each node of the Disk will iterate until the node falls within the unit circle, the total number of iterations $N$ can be found as the sum of the geometric series:
        \begin{align}
            N = \sum_{k=0}^{\infty} n (0.2146)^k = \frac{n}{1-0.2146} = 1.273n
        \end{align}
        where $n = |V|$. This shows this implementation is $\Theta\left(n\right)$.
        \par
        For the node placement algorithm of the Sphere, again the math library in Python makes this easy. Each node needs two random values pulled from a uniform distribution, two square root operations, one sine operation, and one cosine operation. Each node only needs to be placed once so the runtime of this algorithm is $\Theta(n)$ where $n = |V|$.

        \subsubsection{Edge Determination}
        Each method implemented for finding edges has a different time complexity. The brute force method uses an outer loop and an inner loop, which each iterate over every node in the graph. An edge is saved to the adjacency list if the nodes are not the same and the distance between them is less than or equal to the calculated node radius. This is guaranteed to run in $\Theta\left(n^2\right)$ where $n = |V|$. The number of times the distance needs to be calculated is $n \times (n-1)$ because it will not be calculated when the nodes are the same (distance would be zero, but no edge is drawn here). No additional space is needed for the brute force method so the space complexity is $O(1)$.
        \par
        The implementation of sweep starts by sorting the nodes along the x-axis. Python lists have a built-in sort function that has $O\left(n lg(n)\right)$ time complexity \cite{listsort}. After this stage, it iterates over every node building a search space which will be scaned for edges. For each node, the list of nodes is searched right $~r\times n$ nodes to find those within one radius length of the current node. With the search space built, the search space is iterated over once to find nodes that have a distance less than or equal the node radius. Then, the indicies of the nodes are added to the adjacency list entry for each other. My implementation of this runs in $O\left(n lg(n) + 2rn\right)$ where $n = |V|$ and $r$ is the node connection radius. Because the list sort method sorts inplace, the only additional space needed is for the search space. This saves $O(rn)$ nodes and is reset after every iteration.
        \par
        The cell method implementation works in linear time. In the first step of the method, the cells are initialized as a list of empty lists. There are $(1/r + 1)^2$ cells. The nodes are then iterated over and assigned a cell by dividing their x and y coordinates by the node radius. At this point, the cells are iterated over and, for each node in the cell, the nodes in the current cell and the four forward adjacent cells and the are checked to see if they fall within the node radius of the current node. All together, this implementation runs at $O\left(n + n + 5nr^2\right) = O\left((2 + 5r^2)n\right)$ where $n = |V|$. The amount of additional space needed is equal to the number of nodes because they are coppied into their respective cells. This places the space complexity at $\Theta(n)$.

        \subsubsection{Graph Coloring}
        Implementing the smallest-last coloring algorithm involves implementing the smallest-last vertex ordering algorithm and the greedy graph coloring algorithm. For smallest-last vertex ordering, the first thing to do is build the data structure used to represent the graph with deleted nodes. The number of sets needed is equal to the maximum degree of the nodes. Then, the index of each node is placed in the set corresponding to the number of edges it has then the RGG. Simultaneously, a dictionary is created that maps each node to the number of degrees it has in the graph with deletions. Each value starts at the number of edges the corresponding node has in the RGG. At this point, we have iterated over all of the nodes once and allocated space for twice the number of nodes by copying them into the sets and using them as the keys for the degrees dictionary.
        \par
        Because Python dictionaries resize at specific numbers of entries, we can determine the number of additional insertions caused by rehashing while the degrees dictionary is built. Python dictionaries start out with space for 8 entries and quadruple in size until the number of entries is above 50,000, at which point it begins to double in size. Clearly the dictionary grows at a logarithmic rate, but the total number of insertions $I$ for an input size of $n$ is given by:
        \begin{align}
            I =
            \begin{cases}
                n + 8\sum_{k=1}^{\log_4\lceil n/8\rceil}4^k & n \leq 50,000 \\
                n + 8\sum_{k=1}^{6}4^k + 32768\sum_{k=1}^{\log_2\lceil n/32768\rceil}2^k & n > 50,000
            \end{cases}
        \end{align}
        Fortunately, because the entire dictionary is built before it is used by the smallest-last vertex ordering algorithm, it will never again be resized once the algorithm starts. Unfortunately, the sets resize at a similar rate and it is more difficult to predict how large the sets will need to be when performing smallest-last vertex ordering. The degree dictionary will also be used to index into the sets, so we gain a speed up here by not having to iterate over all of the edges for a node and determining if the node it shares an edge with are in the remaining graph each time we want to sift nodes down to lower set.
        \par
        Next, the smallest-last vertex ordering algorithm is run until every node has been removed from the sets. For each node, I iterate over the sets from lowest degree to highest degree to find the first non-empty set. This set must contain the next node to remove becuase it contains all nodes with smallest degree. Before deleting the node from the graph and moving all adjacent nodes down a set, I check to see if the current set has all remaining nodes. If this is the case, the terminal clique has been found, and the size of the terminal clique must be saved. After this check, a node is popped from the end of the current set, and appended to the smallest-last ordering result. Then, for all the adjacent nodes to the popped node in the original graph, I check if the node is in the set with its degree. If it is, the number of degrees for that node can be decremented and the node can be placed into the correct set for its new degree.
        \par
        The last step is to reverse the order of the smallest-last ordering result because it was built in the opposite order (smallest-first). All together, excluding the initialization of accessory data structures, this implementation runs in $\Theta(2|V| + 2|E|)$ time and $\Theta(2|V|)$ space since nodes are removed from the buckets and added to the result.
        \par
        After this the graph needs to be colored. For this I initially assign each node a color of $-1$ in a node color array that is parallel to the original list of nodes. I iterate over all of the nodes in the smallest-last vertex ordering. At each node, I generate a set of colors that is already used by the neighbors of that node by iterating over all of its edge nodes and grabbing their color from the node color array. Then, I just have to increment color from $0$ until it does not exist in the search space set and I have the color to assign to the node.
        \par
        Since the smallest-last odering is used, each time I check to see if a node is adjacent to the current node, I am searching nodes with fewer and fewer edges. This means that the nodes with the most neighbors are searched first, when the number of other nodes to check is lowest, and the nodes with the festest neighbors are searched last, when we have the most nodes to check if they share an edge with the current node. All together, this implementation runs in $\Theta(|V| + 2|E|)$ time and $\Theta(|V|)$ space because we need a new array for the colors assigned to each of the nodes.

    \subsection{Verification}

        \subsubsection{Node Placement}
        The nodes can be verified to be distributed uniformly if the distribution of degrees follows a normal distribution. To show that the distribution of degrees for each of my geometries are following a normal distribution, I plotted degree histograms for each of the geometries with 32,000 nodes and an average degree of 16. The histrogram for Square is given in Figure \ref{squaredeghist}, Disk is given in Figure \ref{diskdeghist}, and Sphere is given in Figure \ref{spheredeghist}. These histograms clearly follow a normal distribution.

        \subsubsection{Edge Determination}
        The runtime for the edge detection methods can be varified by varying the number of nodes and measuring the runtime of each algorithm. By looking at how the runtime grows, we can calculate the trendline that best fits the growth rate. For the first comparison, I vary the number of nodes from 4,000 to 64,000 in steps of 4,000, while holding the desired average dgree constant at 16. As we can see in Figure \ref{avgdeg}, the growth rates of the brute force and sweep methods are quadratic, while the growth rate of the cell method. The trendline functions are given on the graph.
        \par
        For the second metric, I held the number of nodes constant at 32,000 and varied the desired average degree from 2 to 32 in steps of 2. The graph is given in Figure \ref{varavgdeg}. The cell method clearly grows linearly, but the sweep method is harder to gauge. Since varying the desired average degree should only change the node radius, I would expect this to grow linearly as well. However, because each graph is randomly generated, some graphs can have nodes that are closer to sorted order than others. This can effect the measured runtime. It would be easier to gauge the trend if it I ran the data collection multiple times and averaged the results.

        \subsubsection{Graph Coloring}

% \section{Results Summary}

\newpage

\begin{thebibliography}{10}
    \bibitem{spherepoints}
    Weisstein, Eric W., Wolfram MathWorld \\
    \textit{Sphere Point Picking} \\
    \texttt{http://mathworld.wolfram.com/SpherePointPicking.html}

    \bibitem{listsort}
    Peters, Tim \\
    \textit{Timsort} \\
    \texttt{http://svn.python.org/projects/python/trunk/Objects/listsort.txt}

    \bibitem{slv}
    Matula, David and Beck, Leland \\
    \textit{Smallest-Last Ordering and Clustering and Graph Coloring Algorithms}

    \bibitem{ian}
    Johnson, Ian \\
    \textit{Linear-Time Computation of High-Converage Backbones for Wireless Sensor Networks} \\
    \texttt{https://github.com/ianjjohnson/SensorNetwork/blob/master/Report/Report.pdf}

    \bibitem{dictresize}
    Rees, Gareth
    \textit{Python's underlying hash data structure for dictionaries}
    \texttt{https://stackoverflow.com/questions/4279358/pythons-underlying-hash-data-structure-for-dictionaries}

    \bibitem{tupletiming}
    Thomas, Alec
    \textit{Why is tuple faster than list?}
    \texttt{https://stackoverflow.com/questions/3340539/why-is-tuple-faster-than-list}

    \bibitem{fortiming}
    Kruse, Lars
    \textit{Python Speed, Performance Tips}
    \texttt{https://wiki.python.org/moin/PythonSpeed/PerformanceTips}

\end{thebibliography}

\newpage

\section{Appendix A - Figures}

\begin{figure}[h]
    \centering
        \includegraphics[scale=0.6]{./graphs/hist_deg_square.png}
        \caption{Distribution of Degree counts for Square. 32,000 Nodes, Average Degree of 16}
        \label{squaredeghist}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=0.6]{./graphs/hist_deg_disk.png}
    \caption{Distribution of Degree counts for Disk. 32,000 Nodes, Average Degree of 16}
    \label{diskdeghist}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=0.6]{./graphs/hist_deg_sphere.png}
    \caption{Distribution of Degree counts for Sphere. 32,000 Nodes, Average Degree of 16}
    \label{spheredeghist}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=0.6]{./graphs/run_time_avg_deg_16.png}
    \caption{Runtime for Each Edge Detection Method, Average Degree of 16}
    \label{avgdeg}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=0.6]{./graphs/run_time_var_avg_deg.png}
    \caption{Runtime for Cell and Sweep Edge Detection, Variable Average Degree}
    \label{varavgdeg}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=0.6]{./graphs/hist_del_deg.png}
    \caption{Distribution of Degree when Deleted for Square. 32,000 Nodes, Average Degree of 16}
    \label{deldeg}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=0.6]{./graphs/hist_colors.png}
    \caption{Distribution of Colors for Square. 32,000 Nodes, Average Degree of 16}
    \label{distcolors}
\end{figure}

% square images

\begin{figure}
    \begin{minipage}{0.3\textwidth}
    \colorbox{gray}{\includegraphics[width=\linewidth]{./images/square_0.png}}
    \end{minipage}
    \hspace{\fill}
    \begin{minipage}{0.3\textwidth}
    \colorbox{gray}{\includegraphics[width=\linewidth]{./images/square_1.png}}
    \end{minipage}
    \hspace{\fill}
    \begin{minipage}{0.3\textwidth}
    \colorbox{gray}{\includegraphics[width=\linewidth]{./images/square_2.png}}
    \end{minipage}
    \vskip 0.1in
    \begin{minipage}{0.3\textwidth}
    \colorbox{gray}{\includegraphics[width=\linewidth]{./images/square_3.png}}
    \end{minipage}
    \hspace{\fill}
    \begin{minipage}{0.3\textwidth}
    \colorbox{gray}{\includegraphics[width=\linewidth]{./images/square_4.png}}
    \end{minipage}
    \hspace{\fill}
    \begin{minipage}{0.3\textwidth}
    \colorbox{gray}{\includegraphics[width=\linewidth]{./images/square_5.png}}
    \end{minipage}
    \vskip 0.1in
    \begin{minipage}{0.3\textwidth}
    \colorbox{gray}{\includegraphics[width=\linewidth]{./images/square_6.png}}
    \end{minipage}
    \hspace{\fill}

    \caption{Square benchmark graphs}
    \label{squares}
\end{figure}

% disk images

\begin{figure}
    \begin{minipage}{0.45\textwidth}
    \colorbox{gray}{\includegraphics[width=\linewidth]{./images/disk_0.png}}
    \end{minipage}
    \hspace{\fill}
    \begin{minipage}{0.45\textwidth}
    \colorbox{gray}{\includegraphics[width=\linewidth]{./images/disk_1.png}}
    \end{minipage}
    \vskip 0.25in
    \begin{minipage}{0.45\textwidth}
    \colorbox{gray}{\includegraphics[width=\linewidth]{./images/disk_2.png}}
    \end{minipage}

    \caption{Disk benchmark graphs}
    \label{disks}
\end{figure}

% sphere images

\begin{figure}
\begin{minipage}{0.45\textwidth}
    \colorbox{gray}{\includegraphics[width=\linewidth]{./images/sphere_0.png}}
    \end{minipage}
    \hspace{\fill}
    \begin{minipage}{0.45\textwidth}
    \colorbox{gray}{\includegraphics[width=\linewidth]{./images/sphere_1.png}}
    \end{minipage}
    \vskip 0.25in
    \begin{minipage}{0.45\textwidth}
    \colorbox{gray}{\includegraphics[width=\linewidth]{./images/sphere_2.png}}
    \end{minipage}

    \caption{Sphere benchmark graphs}
    \label{spheres}
\end{figure}

\newpage

\section{Appendix B - Code Listings}

\lstset{style=codestyle}

\lstinputlisting[language=Python, caption=Processing driver]{../Processing/Processing.pyde}

\lstinputlisting[language=Python, caption=Topology class and subclasses]{../Processing/objects/topology.py}

\end{document}
